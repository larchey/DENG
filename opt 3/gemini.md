A Multimodal, Graph-Based AI Framework for Scalable Configuration Drift Detection on RHEL EndpointsExecutive SummaryThis report presents a comprehensive, expert-level analysis and a detailed blueprint for a doctorate program at Johns Hopkins University, centered on a project-based, company-sponsored initiative. The core objective is to create a novel AI/ML application for detecting configuration drift on Red Hat Enterprise Linux (RHEL) 8.10 endpoints, leveraging the Elastic stack. The proposed solution moves beyond traditional methods by introducing a multimodal, graph-based approach. The application will use Elastic Agents to collect real-time system data from a large number of endpoints and ingest it into a highly scalable Elasticsearch cluster.The innovation for the academic thesis lies in a unique methodology that combines two advanced AI paradigms: Natural Language Processing (NLP) and Graph Neural Networks (GNNs). This method will semantically analyze an unstructured "approved changes" document (the change's intent) and simultaneously model the live system's state as a graph (the change's reality). The novelty is in the algorithm that compares and reconciles these two disparate data sources to formally identify and score configuration drift.This approach addresses the critical intellectual property (IP) constraint articulated in the user's query: the company retains ownership of the proprietary drift detection application, while the researcher can publish the generalized, novel AI algorithm. The report confirms that this distinction is legally viable and academically sound, providing a clear roadmap for a successful and defensible doctoral dissertation.1. The Problem of Configuration Drift at Scale1.1. Defining Configuration Drift and its Systemic CausesConfiguration drift is the progressive divergence of a system's actual state from its desired, defined, or baseline configuration over time.1 This deviation is not merely a technical flaw; it is a symptom of broader operational and cultural challenges within an organization's IT environment. The primary drivers of drift are human intervention and inconsistent, manual deployment processes.2 Unsanctioned or ad-hoc changes made by system administrators or developers can quickly introduce entropy into a system without proper controls or documentation.3The absence of a standardized, automated deployment process amplifies this issue. If staging and production environments are not configured through a consistent continuous integration/continuous deployment (CI/CD) pipeline, discrepancies are inevitable. The lack of proper version control for configuration files is a significant contributor to this problem, creating a situation where changes are untraceable and undocumented.2 Without a clear history, it becomes impossible to determine who made a change, when it occurred, or why it was implemented.The user's query highlights the existence of an "approved changes document." This document, while a step toward control, can be perceived as a human-centric, reactive mechanism. A critical observation is that such a document, in its unstructured form, represents the intent of a change, but it provides no mechanism to verify the reality of how that change was executed. This distinction reveals that the problem is not a simple binary check of a system's state against a static baseline. Instead, it is a complex challenge of reconciling an approved, intended action with a potentially unintended, unauthorized, or incomplete consequence. A sophisticated solution must be capable of understanding this intent to differentiate an approved modification from a genuine, unapproved configuration drift.1.2. The Business Impacts of Configuration DriftThe ramifications of configuration drift extend far beyond technical inconvenience, creating significant business risks that justify the development of a strategic solution. Configuration drift introduces critical security vulnerabilities, as deviations from secure defaults can create unguarded ports, misconfigured firewalls, or expose outdated software versions, providing potential entry points for malicious actors.2 This lack of consistency circumvents established security measures, elevating the risk of a breach.Furthermore, drift can lead to performance degradation and resource misallocation.2 As settings diverge from optimal configurations, systems may experience reduced throughput or application crashes, disrupting services and impacting user experience.3 From a compliance standpoint, drift can result in serious legal penalties and fines. In industries governed by regulations such as GDPR or HIPAA, the inability to maintain a consistent, auditable configuration state can lead to non-conformance during audits.2Finally, configuration drift makes troubleshooting exceedingly difficult and time-consuming. When system states differ, diagnosing the root cause of an issue becomes a complex, resource-intensive task, leading to increased operational costs and prolonged downtime.2 A simple "revert" function, as suggested by the user's requirement, is insufficient because the negative consequences of drift can occur long before the deviation is detected. This emphasizes that a robust solution must not only detect and correct drift but also provide real-time alerting and proactive root cause analysis to prevent future issues. The project, therefore, serves as a strategic imperative to enhance security, ensure compliance, and reduce operational overhead.2. The Foundation - A Scalable Data Collection and Analysis Platform2.1. Elastic Agent for RHEL 8.10 Endpoint Data CollectionThe selection of the Elastic Agent and its integrations is a technically sound choice for the project. The Elastic Agent serves as a single, unified method for collecting a variety of data types, including logs and metrics, from RHEL 8.10 hosts.5 The agent's ability to operate with superuser permissions is a crucial feature, as it enables access to low-level system data sources that are fundamental to detecting configuration changes.6A key component of this data collection strategy is the Auditd Manager Integration. This integration receives audit events directly from the Linux Audit Framework, which is a core part of the RHEL kernel.7 These auditd logs are a rich source of security-related events and provide granular detail on file modifications, process executions, and user activity—precisely the data required to detect configuration drift.8 The Auditd Manager Integration possesses a unique and valuable capability: it can buffer and combine multiple, interleaved kernel messages related to a single auditable event into a single, cohesive log entry, even if they arrive out of order.7 This feature is critical for data integrity and for accurately reconstructing complex system events, such as file renames, which would be difficult to achieve with manual scripting. This establishes a clear cause-and-effect link: any unapproved change on an endpoint will trigger a series of auditable events, which are then reliably collected and processed by the Elastic Agent and sent to Elasticsearch for analysis.2.2. Designing a Scalable Elasticsearch ClusterThe user's query specifies a "large number of endpoints," which introduces significant scalability challenges for the Elasticsearch cluster. While Elasticsearch is designed to handle high data volumes, a production-grade deployment requires careful architectural planning and configuration.9A fundamental concept for achieving scalability is data sharding and distributed indexing, which allows the workload to be spread across multiple nodes in a cluster.9 To ensure performance and reliability, especially with a high data ingestion rate, the cluster must be properly configured with the correct number of shards and replicas.11 Best practices for handling large-scale data ingestion include using the Elasticsearch Bulk API for efficiency, adding more data nodes to increase processing resources, and temporarily adjusting the refresh interval to improve indexing speed during bulk operations.9Managing a large distributed system also introduces challenges like network latency and maintaining data consistency.12 The proposed solution must address these issues by ensuring low latency between the data nodes and the application, and by configuring shards and replicas to balance performance and data redundancy.11 These architectural considerations are a critical prerequisite for the AI component to function effectively and reliably at scale. The project's success hinges on a robust data foundation, making these design choices a key part of the doctoral research and a justification for the project's practicality.The following table provides a blueprint for the foundational data collection and analysis platform:ComponentConfiguration and RationaleElastic Agent (RHEL)Integration: Use the Auditd Manager Integration.13 Rationale: It reliably captures kernel-level events related to system changes, even combining multi-part messages into single events.7 Permissions: Run with superuser privileges to access all necessary data sources.6 Configuration: Configure auditd rules to specifically monitor critical files, directories, and processes relevant to system configuration.7 This ensures focused and relevant data collection.Elasticsearch ClusterScalability Model: Implement a multi-node cluster with dedicated master, data, and ingest nodes as required for a large-scale deployment. Shards & Replicas: Configure at least three data nodes. Set elasticsearch.defaultShardsPerIndex and elasticsearch.defaultReplicasPerIndex to values that optimize for data volume and redundancy, such as 1 and 2, respectively, for a three-node cluster.11 Indexing: Utilize the Bulk API for efficient data ingestion from a large number of agents.9 Optimize refresh intervals for high-volume data streams to improve indexing performance.9 Performance Tuning: Increase the JVM heap size to 50% of the data node's memory and ensure a low-latency network between all nodes.11Data Ingestion PipelineFlow: Elastic Agents collect auditd logs from RHEL endpoints. The logs are formatted and ingested as JSON documents into Elasticsearch via the Bulk API.9 Data Model: Normalize and parse the logs against a common schema using an ingest pipeline 14, making the data uniform and ready for subsequent AI analysis.3. The Novel AI Method for a Publishable Thesis3.1. Foundational Concepts of AI/ML-based Anomaly DetectionTraditional AI-based anomaly detection models work by establishing a baseline of "normal" behavior and flagging any activity that deviates from this pattern.15 This approach, while effective for many use cases, faces a significant challenge: defining and maintaining an accurate model of "normal" behavior, which is dynamic and complex in nature.15A straightforward anomaly detection model is insufficient for the user's project because it cannot distinguish between an authorized, approved change (an intentional deviation from the old baseline) and an unapproved, malicious, or accidental change (a genuine instance of drift). The "approved changes document" mentioned in the query is a critical, yet unstructured, data source that contains the intent behind a planned system modification. Therefore, the core of the academic research is not simply in detecting an anomaly, but in developing a method to detect a change that contradicts the formally approved intent. This elevates the project from a simple tool to a novel, research-driven solution that provides a robust basis for a doctoral thesis. The solution must be capable of a sophisticated, cross-modal comparison of the documented plan versus the observed reality.3.2. A Multimodal, Graph-Based Approach to Configuration DriftThe proposed doctoral research centers on a novel AI method that integrates two powerful, yet distinct, machine learning paradigms: NLP and GNNs. This multimodal approach provides the framework for a generalized algorithm that can be published without revealing the company's proprietary implementation or data.3.2.1. Module 1: NLP for Approved Changes Document AnalysisThe first module of the proposed method involves using Natural Language Processing to semantically parse the unstructured "approved changes" document. NLP is an ideal tool for this task, as it is designed to analyze unstructured text to identify specialized terms, understand context, and extract entities and their relationships.13 The user's document would contain human-readable information about a change, such as a user (john.doe), an action (install Nginx), and a file path (/etc/nginx.conf).The core of this module's function is to transform this unstructured text into a structured, machine-readable data source, which can be referred to as an "intent graph." This process is analogous to NLP applications in the legal or healthcare fields, where text is analyzed to extract critical information and identify specific concepts and relationships.18 The NLP model would parse the document to create a graph representation with nodes for the user and the application, and a relationship (edge) for the approved action. This intent graph serves as the formal, machine-readable representation of the planned change.3.2.2. Module 2: GNNs for System State ModelingThe second module is a dynamic, real-time representation of the RHEL endpoint's configuration. The intricate, interconnected nature of system components—where users run processes, processes access files, and files contain specific configurations—is a natural fit for a graph data model.20 In this model, system components such as users, files, processes, and network connections become nodes, and their relationships (e.g., "user X is running process Y") become edges.21 The rich, real-time log data collected by the Elastic Agent provides the raw material to build and continuously update this graph representation, or "actual state graph."Graph Neural Networks are a class of deep learning models designed to learn from and operate on exactly this type of graph-structured data.22 The GNN can learn the normal patterns and dependencies of the RHEL system and detect anomalies based on deviations in the graph's structure or node attributes. By modeling the system as a graph, the GNN can uncover subtle patterns and hidden dependencies that a traditional, rule-based detection system would miss.223.2.3. Module 3: Cross-Modal Anomaly DetectionThis is the central innovation of the proposed doctoral thesis. The project is not two separate AI problems but one unified, multimodal one. The "approved changes" NLP model generates the ground truth (the intent graph), and the GNN models the runtime reality (the actual state graph). The core research contribution of the dissertation is the novel algorithm that formally compares and reconciles these two disparate data types.The algorithm would perform a logical comparison, for example: if intent_graph contains (user: john.doe -> action: install) AND actual_state_graph contains (user: john.doe -> action: install), then the change is approved and is not considered drift. Any other change made by that user that is not reflected in the intent graph—such as modifying a file outside the approved scope—would be flagged as a high-confidence instance of configuration drift. The method's novelty resides in this cross-modal fusion and reconciliation. This generalized approach can be published as an abstract algorithm with pseudo-code and formal proofs, without ever revealing the company's proprietary code, data, or product. This strategy satisfies the IP constraint while providing a significant and publishable academic contribution.The following table illustrates the end-to-end data flow and logical components of the proposed AI method:StageData SourceAI/ML ComponentFunctionOutputFinal Anomaly DetectionApproved ChangeUnstructured approved changes documentNatural Language ProcessingSemantic parsing, entity and relationship extractionStructured intent graphComparison & Reconciliation Algorithm: Compares the intent graph against the actual state graph to formally define and score configuration drift.24Live System StateRHEL auditd logs via Elastic AgentGraph Neural NetworkGraph construction, dependency mapping, and anomaly detectionReal-time actual state graphN/A4. The Doctoral Program and IP Strategy4.1. Johns Hopkins Doctoral Program RequirementsThe project-based nature of the user's query aligns well with the academic rigor of a Johns Hopkins doctoral program. Such programs require a minimum of a master's degree and demonstrate outstanding academic achievement.25 The curriculum includes a strong emphasis on quantitative research, statistics, and formal research design.26 A doctoral dissertation must be an original contribution to knowledge, worthy of publication, and defended before a committee.27The proposed multimodal, graph-based AI method provides a solid academic foundation that meets these requirements. The work will not be a simple engineering project but a formal research endeavor supported by academic literature, statistical validation, and a clear, demonstrable contribution to the field. This frames the work as a scholarly pursuit, essential for meeting the university's standards.4.2. Navigating Intellectual Property for a Project-Based DoctorateThe intellectual property (IP) strategy is a critical, non-trivial component of this project. The user's query—to retain company IP for the core tool while publishing a new method—is a sophisticated articulation of a common challenge in corporate-sponsored research. The research material confirms that IP ownership in such scenarios is not straightforward. In general, a student owns IP they create in a course, but IP created as part of a sponsored project is often owned by the university or the sponsoring company.29 The university's policy is that IP created while working in a faculty lab or based on laboratory work is owned by the university.29This nuanced situation requires a formal, pre-emptive legal agreement between the company and the university. A standard sponsored project agreement can stipulate that the sponsoring organization retains exclusive rights to any and all IP resulting from the project.31 However, these agreements can also contain clauses that grant the student permission to publish the project's methods and results in a thesis or journal, as long as it does not compromise the sponsor's competitive advantage.31The proposed solution's structure—separating the proprietary, in-house implementation (the company's IP) from the generalized, publishable algorithm (the thesis IP)—is the key to satisfying all parties. The company would retain ownership of the specific software, data, and models developed to support their product. The researcher would retain the right to publish the novel cross-modal reconciliation algorithm, which can be presented as a general concept with equations and pseudo-code, but without any reference to the proprietary data sets, specific configurations, or business logic. This legal and academic distinction provides the necessary "safe harbor" for the project to proceed successfully.5. Conclusion and Actionable RoadmapThis report demonstrates the feasibility and academic novelty of a doctorate program focused on AI-based configuration drift detection at scale. The project is strategically positioned at the intersection of enterprise IT, advanced AI/ML, and academic research, with a clear path to satisfy all technical, corporate, and educational requirements. The core novelty of a multimodal, graph-based reconciliation algorithm is a robust and publishable contribution to the field.The analysis provides the following actionable roadmap to guide the user's progress:Formalize the IP Agreement: Before beginning any research or development, secure a formal, written agreement with Johns Hopkins University and the company. This agreement must explicitly define the scope of the sponsored project and stipulate that the company retains rights to the proprietary product while the researcher is free to publish the generalized methodology as a doctoral dissertation.Architect the Data Platform: Begin the foundational work of establishing a scalable data platform. This involves properly configuring Elastic Agents with the Auditd Manager Integration on RHEL 8.10 endpoints and designing the Elasticsearch cluster with the necessary sharding, replication, and performance optimizations to handle a large volume of data.Develop the AI Modules: Simultaneously develop the two core AI modules. The first module involves training an NLP model to semantically parse the "approved changes" document and output a structured intent graph. The second module involves building a GNN to dynamically model the live RHEL system state as an actual state graph based on the real-time data collected by the Elastic Agents.Execute the Core Research: The central phase involves the formal development and validation of the novel reconciliation algorithm. This research will focus on the logical and mathematical framework for comparing and scoring the difference between the intent graph and the actual state graph. The results of this algorithm will be the basis for the doctoral thesis, validating its ability to detect genuine configuration drift with high accuracy.Dissertation and Publication: Upon completion of the research, the findings will be documented in a doctoral dissertation and submitted for publication in a peer-reviewed journal. The published work will describe the generalized multimodal, graph-based method and its performance, without revealing any proprietary information related to the company's tool or data.